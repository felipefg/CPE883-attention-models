\documentclass{beamer}
\usepackage{amsfonts,amsmath,oldgerm}
\usetheme{sintef}

\newcommand{\testcolor}[1]{\colorbox{#1}{\textcolor{#1}{test}}~\texttt{#1}}

\usefonttheme[onlymath]{serif}

\titlebackground*{assets/background}

% adicionar o numero na lista final da apresentação
\setbeamertemplate{bibliography item}{\insertbiblabel}

\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

%------------------------------------------------
% Atenção: pressuponho que seu preâmbulo já tenha:
% \documentclass{beamer}
% \usetheme{sintef}
% (e pacotes padrão do template)
%------------------------------------------------

% Metadados (ajuste no preâmbulo do seu .tex)
\title{Attention Models: da motivação às variantes modernas}
\subtitle{Intuição geométrica, formulação matemática e aplicações}
\author{Seu Nome}
\date{\today}

% Opcional: imagem de fundo do título
% \titlebackground{assets/background_negative}

\begin{document}

\maketitle

%===========================
% ROADMAP
%===========================
\section{Roteiro}
\begin{frame}{Roteiro da Apresentação}
\begin{itemize}[<+->]
  \item Motivação e histórico
  \item Modelos de sequência (RNN/LSTM) + Attention
  \item Embeddings, interpretação geométrica e matemática
  \item Transformers e posicionais
  \item Aplicações: Séries Temporais, Language Models, ViT, GAT
  \item Treinamento, variantes e hiperparâmetros
  \item Tamanho ideal (scaling), Mixture of Experts
  \item Pruning e Distillation
  \item Limitações, diagnóstico e conclusões
\end{itemize}
\end{frame}

%===========================
% MOTIVAÇÃO & HISTÓRICO
%===========================
\begin{chapter}[assets/background_negative]{sintefdarkgreen}{Motivação \& Histórico}\end{chapter}

\section{Motivação}
\begin{frame}{Por que Atenção?}
\begin{itemize}[<+->]
  \item Capturar dependências longas e filtrar informação relevante
  \item RNNs sofrem com gradientes e paralelização limitada
  \item Pesos de atenção ajudam na interpretabilidade
  \item Melhor relação capacidade/compute em diversas tarefas
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Histórico em 3 passos}
\begin{itemize}[<+->]
  \item \textbf{Additive Attention} (Bahdanau) para NMT
  \item \textbf{Dot-Product Attention} (Luong) — caminho para escalabilidade
  \item \textbf{Transformers} (Vaswani et al., 2017): “Attention is All You Need”
\end{itemize}
% Tempo ~1 min
\end{frame}

%===========================
% FUNDAMENTOS
%===========================
\begin{chapter}[assets/background_negative]{sintefgreen}{Fundamentos}\end{chapter}

\section{Modelos de Sequência}
\begin{frame}{RNN/GRU/LSTM — limites práticos}
\begin{itemize}[<+->]
  \item Cálculo sequencial $\Rightarrow$ baixa paralelização
  \item Dependências longas: vanish/explode (mitigado, não resolvido)
  \item Memória finita e custo de treino elevado para contextos longos
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{LSTM + Attention (Encoder--Decoder)}
\framesubtitle{Atenção aditiva (Bahdanau)}
\begin{block}{Equações}
\small
\[
e_{ij} = v^\top \tanh\!\big(W_1\,h_i + W_2\,s_j\big), \quad
\alpha_{ij} = \mathrm{softmax}_i(e_{ij}), \quad
c_j = \sum_i \alpha_{ij} h_i
\]
\end{block}
\begin{itemize}[<+->]
  \item \textbf{Alinhamento} dinâmico entre estados do encoder e passos do decoder
  \item Reduz gargalos de um único vetor de contexto
\end{itemize}
% Tempo ~2 min
\end{frame}

\section{Embeddings \& Interpretações}
\begin{frame}{Word/Subword Embeddings}
\begin{itemize}[<+->]
  \item Estáticos (Word2Vec, GloVe) vs. Contextuais (ELMo, BERT)
  \item Subword (BPE/Unigram) para robustez morfológica
  \item Análogos em outras modalidades: patches (ViT), time2vec (TS), node2vec (grafos)
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Interpretação Geométrica da Atenção}
\begin{itemize}[<+->]
  \item $Q,K,V$ são projeções lineares do latente
  \item Similaridade (coseno/dot) guia uma \emph{combinação convexa} de $V$
  \item Multi-head $\Rightarrow$ múltiplas métricas/projeções simultâneas
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Interpretação Matemática: Self-Attention}
\begin{block}{Fórmula central (scaled dot-product)}
\[
\mathrm{Att}(Q,K,V) \;=\; \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)\,V
\]
\end{block}
\begin{itemize}[<+->]
  \item Máscara causal/atencional conforme a tarefa
  \item Complexidade $\mathcal{O}(n^2)$ em tempo/memória
  \item Gradientes e saturação da softmax
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Positional Encodings}
\begin{itemize}[<+->]
  \item Absolutos senoidais vs. aprendidos
  \item Relativos e RoPE (rotary): melhor generalização/composição
  \item Impacto em extrapolação e contextos longos
\end{itemize}
% Tempo ~2 min
\end{frame}

%===========================
% TRANSFORMERS
%===========================
\begin{chapter}[assets/background_negative]{sinteflightgreen}{Transformers}\end{chapter}

\section{Arquiteturas \& Variantes}
\begin{frame}{Arquiteturas Transformer}
\begin{itemize}[<+->]
  \item \textbf{Encoder--Decoder}, \textbf{Encoder-only}, \textbf{Decoder-only}
  \item Bloco: MHSA + FFN, \textit{residual} + \textit{layer norm}
  \item Pré-norm vs. pós-norm (estabilidade de treino)
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Atenções Eficientes}
\begin{itemize}[<+->]
  \item \textbf{Esparsidade}: Longformer/BigBird (padrões locais+globais)
  \item \textbf{Aproximação}: Linformer, Nyströmformer
  \item \textbf{Kernels}: Performer (favor+), FlashAttention (IO-aware)
  \item \textbf{Outros}: Reformer (LSH)
\end{itemize}
\begin{block}{Trade-off}
Complexidade $\mathcal{O}(n^2)$ $\rightarrow$ $\tilde{\mathcal{O}}(n)$ com perda controlada de exatidão e/ou restrições de padrão.
\end{block}
% Tempo ~3 min
\end{frame}

\section{Treino \& Hiperparâmetros}
\begin{frame}{Práticas de Treino}
\begin{itemize}[<+->]
  \item AdamW, warmup + decaimento; label smoothing quando aplicável
  \item Regularização: dropout, stochastic depth, weight decay
  \item AMP/mixed precision, grad clipping, checkpointing
  \item Dados: curriculum, masking, augmentation (TS/ViT/GAT)
\end{itemize}
% Tempo ~3 min
\end{frame}

\begin{frame}{Hiperparâmetros Essenciais}
\begin{itemize}[<+->]
  \item Profundidade, $d_{\text{model}}$, \#heads, $d_{ff}$, dropout
  \item Comprimento de contexto, batch size, \textit{LR schedule}
  \item Específicos: tokenização (LM), \textit{patch size} (ViT), janela/patch (TS)
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Qual tamanho ideal? (Scaling)}
\begin{itemize}[<+->]
  \item Leis de escala vs. limite de dados/compute
  \item Tokens vs. parâmetros; saturação com contexto
  \item Regra prática: dimensione para o dataset e o \textit{budget} de inferência
\end{itemize}
% Tempo ~3 min
\end{frame}

%===========================
% APLICAÇÕES
%===========================
\begin{chapter}[assets/background_negative]{sintefyellow}{Aplicações}\end{chapter}

\section{Séries Temporais}
\begin{frame}{Atenção em Séries Temporais}
\begin{itemize}[<+->]
  \item Codificação temporal: absolutos/relativos, time2vec, calendários/sazonalidade
  \item Exógenas e \textit{cross-attention}; \textit{patching} para contextos longos
  \item Tarefas: previsão, imputação, detecção de anomalias
\end{itemize}
% Tempo ~3 min
\end{frame}

\section{Language Models}
\begin{frame}{Language Models (LM)}
\begin{itemize}[<+->]
  \item Atenção \textbf{causal}, \textit{next-token} e \textit{masking}
  \item Pré-treino vs. \textit{fine-tuning}; Instrução/LoRA/Adapters
  \item Métricas: perplexidade e tarefas \textit{downstream}
\end{itemize}
% Tempo ~2 min
\end{frame}

\section{Vision Transformer}
\begin{frame}{Vision Transformer (ViT)}
\begin{itemize}[<+->]
  \item Imagem $\rightarrow$ \textit{patches} + \texttt{[CLS]} token
  \item Posicionais 2D; augmentations (RandAug, Mixup/CutMix)
  \item Transfer: \textit{linear probe} vs. \textit{fine-tune}
\end{itemize}
% Tempo ~2 min
\end{frame}

\section{Graph Attention}
\begin{frame}{Graph Attention Networks (GAT)}
\begin{block}{Coeficientes de Atenção (um cabeçalho)}
\small
\[
\alpha_{ij}=\mathrm{softmax}_j\!\left(\mathrm{LeakyReLU}\!\big(a^\top[Wh_i \,||\, Wh_j]\big)\right)
\]
\end{block}
\begin{itemize}[<+->]
  \item Multi-head; sobre-\textit{smoothing} e escalabilidade
  \item Heterógrafos e atenção relacional
\end{itemize}
% Tempo ~2 min
\end{frame}

%===========================
% ESCALA & COMPRESSÃO
%===========================
\begin{chapter}[assets/background_negative]{sintefred}{Escala \& Compressão}\end{chapter}

\section{MoE, Pruning, Distillation}
\begin{frame}{Mixture of Experts (MoE)}
\begin{itemize}[<+->]
  \item \textit{Gating} top-1/top-2; balanceamento de carga
  \item Roteamento esparso: capacidade vs. ociosidade
  \item Custos de comunicação e estabilidade no treino
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Pruning}
\begin{itemize}[<+->]
  \item Não estruturado (magnitude) vs. estruturado (canal/bloco, n:m)
  \item Iterativo vs. \textit{one-shot}; impacto em latência real
  \item Interação com quantização e \textit{sparsity-aware kernels}
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Distillation}
\begin{block}{Perda típica (Hinton)}
\small
\[
\mathcal{L}=(1-\lambda)\,\mathrm{CE}(y,s)+\lambda\,T^2\,\mathrm{KL}\!\big(p_T \,\|\, q_T\big)
\]
\end{block}
\begin{itemize}[<+->]
  \item \textit{Teacher} $\rightarrow$ \textit{student}; temperatura $T$
  \item \textit{Intermediate layer hints}; \textit{task-specific} vs. generalista
  \item Benefícios: latência/energia e \textit{edge deployment}
\end{itemize}
% Tempo ~2 min
\end{frame}

%===========================
% INTERPRETAÇÃO & LIMITES
%===========================
\begin{chapter}[assets/background_negative]{sinteflilla}{Interpretação \& Limitações}\end{chapter}

\section{Diagnóstico \& Limites}
\begin{frame}{Interpretação \& Diagnóstico}
\begin{itemize}[<+->]
  \item \textit{Attention rollout/flow}; atenção $\neq$ causalidade
  \item \textit{Probing} de camadas; ablação de cabeças
  \item Ferramentas de \textit{explainability} por domínio
\end{itemize}
% Tempo ~2 min
\end{frame}

\begin{frame}{Limitações \& Trade-offs}
\begin{itemize}[<+->]
  \item Custo $\mathcal{O}(n^2)$, viés de dados, \textit{OOD} robustness
  \item Contaminação de treino e privacidade
  \item Energia/carbono e restrições de hardware
\end{itemize}
% Tempo ~2 min
\end{frame}

%===========================
% CONCLUSÕES
%===========================
\section{Conclusões}
\begin{frame}{Conclusões}
\begin{itemize}[<+->]
  \item Atenção como princípio unificador em sequência, visão e grafos
  \item Escolha guiada por dados, métricas e orçamento (treino/inferência)
  \item Escala e compressão (MoE/pruning/distillation) para produção
\end{itemize}
% Tempo ~1 min
\end{frame}

%===========================
% Q&A
%===========================
\backmatter
\begin{frame}
\frametitle{Perguntas?}
\centering
Obrigado!
\end{frame}

%===========================
% BACKUP (opc.)
%===========================
\section{Backup (Opcional)}
\begin{frame}{Equações úteis (resumo)}
\small
\begin{itemize}
  \item Scaled dot-product, aditiva (Bahdanau)
  \item Atenção relativa e RoPE
  \item GAT detalhado; máscaras causais
\end{itemize}
\end{frame}

\begin{frame}{Hiperparâmetros por tarefa (resumo)}
\small
\begin{itemize}
  \item LM: contexto, BPE, heads/profundidade típicos
  \item ViT: patch size, MLP ratio, augmentations
  \item TS: janela, patching, covariáveis, perdas (MAE/MSE/Quantile)
\end{itemize}
\end{frame}

%===========================
% REFERÊNCIAS
%===========================
\section{Referências Bibliográficas} 
\begin{frame}[allowframebreaks]
  \frametitle{Referências Bibliográficas} 
  \bibliographystyle{ieeetr}
  \bibliography{presentation_bib.bib}
\end{frame}

\end{document}

